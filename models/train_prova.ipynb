{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import argparse\n",
    "from collections import defaultdict\n",
    "import logging\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.utils import shuffle\n",
    "import sys\n",
    "import yaml\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch.optim as optim\n",
    "import h5py\n",
    "import logging\n",
    "import sys\n",
    "from tqdm import tqdm\n",
    "\n",
    "from ops_pytorch import (minibatch_discriminator, minibatch_output_shape, Dense3D,\n",
    "                     calculate_energy, scale, InpaintingAttention)\n",
    "\n",
    "from architectures_torch import build_Generator, build_Discriminator\n",
    "\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Settaggio Parametri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "[['pion', '../data/piplus.hdf5']]\n"
     ]
    }
   ],
   "source": [
    "nb_epochs = 50\n",
    "batch_size = 256\n",
    "latent_size = 1024\n",
    "verbose = True\n",
    "no_attn = False\n",
    "\n",
    "disc_lr =2e-5\n",
    "gen_lr = 2e-4\n",
    "adam_beta_1 = 0.5\n",
    "\n",
    "yaml_file = \"particles.yaml\"\n",
    "\n",
    "    # read in data file spec from YAML file\n",
    "with open(yaml_file, 'r') as stream:\n",
    "    try:\n",
    "        s = yaml.safe_load(stream)\n",
    "    except yaml.YAMLError as exc:\n",
    "\n",
    "        raise exc\n",
    "nb_classes = len(s.keys())\n",
    "print(nb_classes)\n",
    "a=[[p,f] for p,f in s.items()]\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Caricamento dati "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bit_flip(x, prob=0.05):\n",
    "    \"\"\" flips a int array's values with some probability \"\"\"\n",
    "    x = np.array(x)\n",
    "    selection = np.random.uniform(0, 1, x.shape) < prob\n",
    "    x[selection] = 1 * np.logical_not(x[selection])\n",
    "    return x\n",
    "\n",
    "def _load_data(particle, datafile):\n",
    "\n",
    "\n",
    "        d = h5py.File(datafile, 'r')\n",
    "\n",
    "        # make our calo images channels-last\n",
    "        first = np.expand_dims(d['layer_0'][:], -1)\n",
    "        second = np.expand_dims(d['layer_1'][:], -1)\n",
    "        third = np.expand_dims(d['layer_2'][:], -1)\n",
    "        # convert to MeV\n",
    "        energy = d['energy'][:].reshape(-1, 1) * 1000\n",
    "\n",
    "        sizes = [\n",
    "            first.shape[1], first.shape[2],\n",
    "            second.shape[1], second.shape[2],\n",
    "            third.shape[1], third.shape[2]\n",
    "        ]\n",
    "\n",
    "        y = [particle] * first.shape[0]\n",
    "\n",
    "        d.close()\n",
    "\n",
    "        return first, second, third, y, energy, sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([256, 1, 3, 96])\n",
      "torch.Size([256, 1, 12, 12])\n",
      "torch.Size([256, 1, 12, 6])\n",
      "torch.Size([256, 1])\n",
      "torch.Size([256])\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n"
     ]
    }
   ],
   "source": [
    "first, second, third, y, energy, sizes = [\n",
    "    np.concatenate(t) for t in [\n",
    "        a for a in zip(*[_load_data(p, f) for p, f in s.items()])\n",
    "    ]\n",
    "]\n",
    "\n",
    "sizes = sizes[:6].tolist()\n",
    "\n",
    "# scale the energy depositions by 1000 to convert MeV => GeV\n",
    "first, second, third, energy = [\n",
    "    (X.astype(np.float32) / 1000)\n",
    "    for X in [first, second, third, energy]\n",
    "    ]\n",
    "le = LabelEncoder()\n",
    "y = le.fit_transform(y)\n",
    "\n",
    "first = torch.tensor(first, dtype=torch.float32).permute(0, 3, 1, 2)\n",
    "second = torch.tensor(second, dtype=torch.float32).permute(0, 3, 1, 2)\n",
    "third = torch.tensor(third, dtype=torch.float32).permute(0, 3, 1, 2)\n",
    "y = torch.tensor(y, dtype=torch.long)\n",
    "energy = torch.tensor(energy, dtype=torch.float32, requires_grad=True)\n",
    "\n",
    "# Dataset and DataLoader\n",
    "dataset = TensorDataset(first, second, third, energy,y)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "x=next(iter(dataloader))\n",
    "print(x[0].shape)\n",
    "print(x[1].shape)\n",
    "print(x[2].shape)\n",
    "print(x[3].shape)\n",
    "print(x[4].shape)\n",
    "print(x[4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discriminatore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, sizes, nb_classes=1):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.nb_classes = nb_classes\n",
    "        self.mbd = True\n",
    "        self.sparsity = True\n",
    "        self.sparsity_mbd = True\n",
    "\n",
    "        self.layers = nn.ModuleList([\n",
    "            build_Discriminator(mbd=self.mbd, sparsity=self.sparsity, sparsity_mbd=self.sparsity_mbd,sizes=sizes[2*i:(i+1)*2])\n",
    "            for i in range(3)\n",
    "        ])\n",
    "\n",
    "#NOTE: con tutto true il fc ha bisogno di 1863, mentre nel caso false solo 1800\n",
    "        self.fc = nn.Linear(1863, 1) \n",
    "        self.aux_fc = nn.Linear(1863, 1) if nb_classes > 1 else None\n",
    "\n",
    "        # For minibatch discrimination\n",
    "        self.dense3d_energy = Dense3D(first_dim=10, last_dim=10, input_shape=(3,1))\n",
    "\n",
    "    def forward(self, inputs, input_energy):\n",
    "        features = []\n",
    "        energies = []\n",
    "\n",
    "        # Extract features and energies from each calorimeter layer\n",
    "        for i, input in enumerate(inputs):\n",
    "            features.append(self.layers[i](input))\n",
    "            energies.append(calculate_energy(input))\n",
    "\n",
    "        # Concatenate features\n",
    "        features = torch.cat(features, dim=1)\n",
    "        energies = torch.cat(energies,dim=1)\n",
    "\n",
    "        # Total energy across all rows\n",
    "        total_energy = torch.sum(energies, dim=1, keepdim=True)\n",
    "\n",
    "        # Minibatch discrimination on the raw energies\n",
    "        K_energy = self.dense3d_energy(energies)\n",
    "        mbd_energy = torch.tanh(minibatch_discriminator(K_energy))\n",
    "\n",
    "        # Absolute deviation from input energy\n",
    "        energy_well = torch.abs(total_energy - input_energy)\n",
    "        \n",
    "        # Binary y/n if it is over the input energy\n",
    "        well_too_big = 10 * (energy_well > 5).float()\n",
    "\n",
    "        # Concatenate all features\n",
    "        p = torch.cat([\n",
    "            features,\n",
    "            scale(energies, 10),\n",
    "            scale(total_energy, 100),\n",
    "            energy_well,\n",
    "            well_too_big,\n",
    "            mbd_energy\n",
    "        ], dim=1)\n",
    "\n",
    "        fake = torch.sigmoid(self.fc(p))\n",
    "        discriminator_outputs = [fake, total_energy]\n",
    "\n",
    "        # Auxiliary output for ACGAN\n",
    "        if self.nb_classes > 1:\n",
    "            aux = torch.sigmoid(self.aux_fc(p))\n",
    "            discriminator_outputs.append(aux)\n",
    "\n",
    "        return discriminator_outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generatore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self, latent_size, no_attn, nb_classes=1):\n",
    "        super(Generator, self).__init__()\n",
    "        self.latent_size = latent_size\n",
    "        self.nb_classes = nb_classes\n",
    "        self.no_attn = no_attn\n",
    "\n",
    "        # Embedding layer\n",
    "        if nb_classes > 1:\n",
    "            self.embedding = nn.Embedding(nb_classes, latent_size)\n",
    "            self.flatten = nn.Flatten()\n",
    "\n",
    "\n",
    "        # Define generator layers\n",
    "        self.gen_layer0 = build_Generator(latent_size, 3, 96)\n",
    "        self.gen_layer1 = build_Generator(latent_size, 12, 12)\n",
    "        self.gen_layer2 = build_Generator(latent_size, 12, 6)\n",
    "\n",
    "        if not no_attn:\n",
    "            self.attn_layer1=InpaintingAttention(constant=-10.0, input_size=[14,14])\n",
    "            self.attn_layer2=InpaintingAttention(constant=-10.0, input_size=[14,8])\n",
    "\n",
    "\n",
    "    def forward(self, generator_inputs, image_class=None):\n",
    "        latent=generator_inputs[0]\n",
    "        input_energy=generator_inputs[1]\n",
    "        if self.nb_classes > 1 and image_class is not None:\n",
    "            emb = self.embedding(image_class)\n",
    "            emb = self.flatten(emb)\n",
    "            hc = latent * emb\n",
    "            h = hc * scale(input_energy, 100)\n",
    "        else:\n",
    "            h = latent * scale(input_energy, 100).shape[1]\n",
    "\n",
    "        img_layer0 = self.gen_layer0(h)\n",
    "        img_layer1 = self.gen_layer1(h)\n",
    "        img_layer2 = self.gen_layer2(h)\n",
    "\n",
    "        if not no_attn:\n",
    "            # resizes from (3, 96) => (12, 12)\n",
    "            zero2one = nn.AvgPool2d(kernel_size=(1, 8))(\n",
    "                nn.Upsample(scale_factor=(4, 1), mode='nearest')(img_layer0))\n",
    "            img_layer1 = self.attn_layer1(img_layer1, zero2one)\n",
    "            \n",
    "            # resizes from (12, 12) => (12, 6)\n",
    "            one2two = nn.AvgPool2d(kernel_size=(1, 2))(img_layer1)\n",
    "            img_layer2 = self.attn_layer2(img_layer2, one2two)\n",
    "     \n",
    "\n",
    "        return [F.relu(img_layer0), F.relu(img_layer1), F.relu(img_layer2)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combined model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "latent torch.Size([256, 1024])\n",
      "input torch.Size([256, 1])\n",
      "torch.Size([256])\n",
      "True\n",
      "fake torch.Size([256, 1])\n",
      "energy torch.Size([256, 1])\n"
     ]
    }
   ],
   "source": [
    "inputs = [3,96,12,12,12,6]\n",
    "discriminator = Discriminator(inputs,2)\n",
    "\n",
    "\n",
    "for param in discriminator.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "latent = torch.randn(batch_size, latent_size)  # Example latent input\n",
    "input_energy = torch.randn((batch_size, 1)) \n",
    "generator_inputs = [latent, input_energy] \n",
    "print(\"latent\", latent.shape)\n",
    "print(\"input\",input_energy.shape)\n",
    "\n",
    "generator=Generator(latent_size, False)\n",
    "out=generator(generator_inputs)\n",
    "#combined_input = [torch.cat([out[i], input_energy], dim=0) for i in range(3)]\n",
    "#print(out[0].shape)\n",
    "sampled_energies = torch.rand( size=(batch_size, 1))*99+1\n",
    "out2=discriminator(out, sampled_energies)\n",
    "print(out2[0].view(-1).shape)\n",
    "print(out2[2].requires_grad)\n",
    "\n",
    "\n",
    "class CombinedModel(nn.Module):\n",
    "    def __init__(self, generator, discriminator):\n",
    "        super(CombinedModel, self).__init__()\n",
    "        self.generator = generator\n",
    "        self.discriminator = discriminator\n",
    "\n",
    "    def forward(self, generator_inputs, image_class=None):\n",
    "        latent=generator_inputs[0]\n",
    "        input_energy=generator_inputs[1]\n",
    "        gen_output = self.generator(generator_inputs, image_class)\n",
    "        disc_output = self.discriminator(gen_output, input_energy)\n",
    "        return disc_output\n",
    "\n",
    "# Instantiate the combined model\n",
    "\n",
    "\n",
    "combined = CombinedModel(generator, discriminator)\n",
    "out=combined(generator_inputs)\n",
    "print(\"fake\",out[0].shape)\n",
    "print(\"energy\", out[1].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SaveBestModel:\n",
    "    def __init__(self, best_valid_loss=float('inf')): #object initialized with best_loss = +infinite\n",
    "        self.best_valid_loss = best_valid_loss\n",
    "\n",
    "    def __call__(\n",
    "        self, current_valid_loss,\n",
    "        epoch, model, optimizer, criterion, metric\n",
    "    ):\n",
    "        if current_valid_loss < self.best_valid_loss:\n",
    "            self.best_valid_loss = current_valid_loss\n",
    "\n",
    "            print(f\"\\nBest validation loss: {self.best_valid_loss}\")\n",
    "            print(f\"\\nSaving best model for epoch: {epoch+1}\\n\")\n",
    "\n",
    "            # method to save a model (the state_dict: a python dictionary object that\n",
    "            # maps each layer to its parameter tensor) and other useful parametrers\n",
    "            # see: https://pytorch.org/tutorials/beginner/saving_loading_models.html\n",
    "\n",
    "            torch.save({'model' : model,\n",
    "                'epoch': epoch+1,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'loss': criterion,'metric': metric,\n",
    "                }, 'best_model.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt_g = torch.optim.Adam(params=generator.parameters(), lr=gen_lr, weight_decay=1e-5)\n",
    "opt_d = torch.optim.Adam(params=discriminator.parameters(), lr=disc_lr, weight_decay=1e-5)\n",
    "\n",
    "bce_loss=nn.BCELoss()\n",
    "mae_loss=nn.L1Loss()\n",
    "\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "generator.to(device)\n",
    "discriminator.to(device)\n",
    "\n",
    "\n",
    "\n",
    "model=CombinedModel(generator, discriminator)\n",
    "model.to(device)\n",
    "\n",
    "disc_loss = []\n",
    "gen_loss = []\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.2187])\n",
      "[1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n"
     ]
    }
   ],
   "source": [
    "sampled_energies = torch.rand( size=(batch_size, 1))*99+1\n",
    "print(min(sampled_energies))\n",
    "\n",
    "loss_weights = [np.ones(batch_size), 0.05 * np.ones(batch_size)]\n",
    "print(loss_weights[0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "nb_epochs=1\n",
    "for epoch in range(nb_epochs):\n",
    "    t0 = time.time()\n",
    "    train_loss = 0\n",
    "    counter=0\n",
    "    for image_batch_1,image_batch_2, image_batch_3, energy_batch, label_batch in dataloader:\n",
    "\n",
    "        noise = torch.normal(0, 1, size=(batch_size, latent_size), requires_grad=True)\n",
    "\n",
    "        # energy_breakdown\n",
    "\n",
    "        sampled_labels = torch.randint(0, nb_classes, size=(batch_size,))\n",
    "        sampled_energies = torch.rand( size=(batch_size, 1))*99+1\n",
    "\n",
    "        generator_inputs = [noise, sampled_energies]\n",
    "\n",
    "        \"\"\" if nb_classes > 1:\n",
    "                # in the case of the ACGAN, we need to append the requested\n",
    "                # class to the pre-image of the generator\n",
    "                generator_inputs.append(sampled_labels) \"\"\"\n",
    "\n",
    "        generated_images = generator(generator_inputs)\n",
    "\n",
    "        #disc_outputs_real = [torch.ones(batch_size), energy_batch]\n",
    "        #disc_outputs_fake = [torch.zeros(batch_size), sampled_energies]\n",
    "\n",
    "        #loss_weights = torch.Tensor([1, 0.05]\n",
    "        #print(loss_weights.requires_grad)\n",
    "\n",
    "        \n",
    "        \n",
    "        out = discriminator([image_batch_1,image_batch_2, image_batch_3], energy_batch)\n",
    "        print(out[0].requires_grad)\n",
    "        loss_real =   bce_loss(out[0].view(-1), torch.ones(batch_size)) + 0.05 * mae_loss(out[1].view(-1), energy_batch)\n",
    "        \n",
    "        opt_d.zero_grad()\n",
    "        loss_real.backward()\n",
    "        opt_d.step()\n",
    "\n",
    "        out = discriminator(generated_images, sampled_energies)\n",
    "        loss_fake =  bce_loss(out[0].view(-1), torch.zeros(batch_size)) + 0.05*mae_loss(out[1].view(-1), sampled_energies)\n",
    "        \n",
    "        opt_d.zero_grad()\n",
    "        loss_fake.backward()\n",
    "        opt_d.step()\n",
    "\n",
    "        disc_loss.append((loss_real.item() + loss_fake.item()) / 2)\n",
    "        \n",
    "       \n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import (Activation, AveragePooling2D, Dense, Embedding,\n",
    "                             Flatten, Input, Lambda, UpSampling2D)\n",
    "latent = Input(shape=(latent_size, ), name='z')\n",
    "input_energy = Input(shape=(1, ), dtype='float32')\n",
    "generator_inputs = [latent, input_energy]\n",
    "\n",
    "print(latent.shape)\n",
    "print(input_energy.shape)\n",
    "\n",
    "h = Lambda(lambda x: x[0] * x[1])([latent, scale(input_energy, 100)])\n",
    "print(h.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
