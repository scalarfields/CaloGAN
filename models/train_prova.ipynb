{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import argparse\n",
    "from collections import defaultdict\n",
    "import logging\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.utils import shuffle\n",
    "import sys\n",
    "import yaml\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch.optim as optim\n",
    "import h5py\n",
    "import logging\n",
    "import sys\n",
    "from tqdm import tqdm\n",
    "\n",
    "from ops_pytorch import (minibatch_discriminator, minibatch_output_shape, Dense3D,\n",
    "                     calculate_energy, scale, InpaintingAttention)\n",
    "\n",
    "from architectures_torch import build_Generator, build_Discriminator\n",
    "\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Settaggio Parametri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "[['positron', '../data/eplus.hdf5']]\n"
     ]
    }
   ],
   "source": [
    "nb_epochs = 50\n",
    "batch_size = 256\n",
    "latent_size = 100\n",
    "verbose = True\n",
    "no_attn = True\n",
    "\n",
    "disc_lr =2e-5\n",
    "gen_lr = 2e-4\n",
    "adam_beta_1 = 0.5\n",
    "\n",
    "yaml_file = \"particles.yaml\"\n",
    "\n",
    "    # read in data file spec from YAML file\n",
    "with open(yaml_file, 'r') as stream:\n",
    "    try:\n",
    "        s = yaml.safe_load(stream)\n",
    "    except yaml.YAMLError as exc:\n",
    "\n",
    "        raise exc\n",
    "nb_classes = len(s.keys())\n",
    "print(nb_classes)\n",
    "a=[[p,f] for p,f in s.items()]\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Caricamento dati "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bit_flip(x, prob=0.05):\n",
    "    \"\"\" flips a int array's values with some probability \"\"\"\n",
    "    x = np.array(x)\n",
    "    selection = np.random.uniform(0, 1, x.shape) < prob\n",
    "    x[selection] = 1 * np.logical_not(x[selection])\n",
    "    return x\n",
    "\n",
    "def _load_data(particle, datafile):\n",
    "\n",
    "\n",
    "        d = h5py.File(datafile, 'r')\n",
    "\n",
    "        # make our calo images channels-last\n",
    "        first = np.expand_dims(d['layer_0'][:], -1)\n",
    "        second = np.expand_dims(d['layer_1'][:], -1)\n",
    "        third = np.expand_dims(d['layer_2'][:], -1)\n",
    "        # convert to MeV\n",
    "        energy = d['energy'][:].reshape(-1, 1) * 1000\n",
    "\n",
    "        sizes = [\n",
    "            first.shape[1], first.shape[2],\n",
    "            second.shape[1], second.shape[2],\n",
    "            third.shape[1], third.shape[2]\n",
    "        ]\n",
    "\n",
    "        y = [particle] * first.shape[0]\n",
    "\n",
    "        d.close()\n",
    "\n",
    "        return first, second, third, y, energy, sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([256, 1, 3, 96])\n",
      "torch.Size([256, 1, 12, 12])\n",
      "torch.Size([256, 1, 12, 6])\n",
      "torch.Size([256, 1])\n",
      "torch.Size([256])\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n"
     ]
    }
   ],
   "source": [
    "first, second, third, y, energy, sizes = [\n",
    "    np.concatenate(t) for t in [\n",
    "        a for a in zip(*[_load_data(p, f) for p, f in s.items()])\n",
    "    ]\n",
    "]\n",
    "\n",
    "sizes = sizes[:6].tolist()\n",
    "\n",
    "# scale the energy depositions by 1000 to convert MeV => GeV\n",
    "first, second, third, energy = [\n",
    "    (X.astype(np.float32) / 1000)\n",
    "    for X in [first, second, third, energy]\n",
    "    ]\n",
    "le = LabelEncoder()\n",
    "y = le.fit_transform(y)\n",
    "\n",
    "first = torch.tensor(first, dtype=torch.float32)[:10000].permute(0, 3, 1, 2)\n",
    "second = torch.tensor(second, dtype=torch.float32)[:10000].permute(0, 3, 1, 2)\n",
    "third = torch.tensor(third, dtype=torch.float32)[:10000].permute(0, 3, 1, 2)\n",
    "y = torch.tensor(y, dtype=torch.long)[:10000]\n",
    "energy = torch.tensor(energy, dtype=torch.float32, requires_grad=True)[:10000]\n",
    "# Dataset and DataLoader\n",
    "dataset = TensorDataset(first, second, third, energy,y)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "x=next(iter(dataloader))\n",
    "print(x[0].shape)\n",
    "print(x[1].shape)\n",
    "print(x[2].shape)\n",
    "print(x[3].shape)\n",
    "print(x[4].shape)\n",
    "print(x[4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discriminatore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, sizes, nb_classes=1):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.nb_classes = nb_classes\n",
    "        self.mbd = True\n",
    "        self.sparsity = True\n",
    "        self.sparsity_mbd = True\n",
    "\n",
    "        self.layers = nn.ModuleList([\n",
    "            build_Discriminator(mbd=self.mbd, sparsity=self.sparsity, sparsity_mbd=self.sparsity_mbd,sizes=sizes[2*i:(i+1)*2])\n",
    "            for i in range(3)\n",
    "        ])\n",
    "\n",
    "#NOTE: con tutto true il fc ha bisogno di 1863, mentre nel caso false solo 1800\n",
    "        self.fc = nn.Linear(1863, 1) \n",
    "        self.aux_fc = nn.Linear(1863, 1) if nb_classes > 1 else None\n",
    "\n",
    "        # For minibatch discrimination\n",
    "        self.dense3d_energy = Dense3D(first_dim=10, last_dim=10, input_shape=(3,1))\n",
    "\n",
    "    def forward(self, inputs, input_energy):\n",
    "        features = []\n",
    "        energies = []\n",
    "\n",
    "        # Extract features and energies from each calorimeter layer\n",
    "        for i, input in enumerate(inputs):\n",
    "            features.append(self.layers[i](input))\n",
    "            energies.append(calculate_energy(input))\n",
    "\n",
    "        # Concatenate features\n",
    "        features = torch.cat(features, dim=1)\n",
    "        energies = torch.cat(energies,dim=1)\n",
    "\n",
    "        # Total energy across all rows\n",
    "        total_energy = torch.sum(energies, dim=1, keepdim=True)\n",
    "\n",
    "        # Minibatch discrimination on the raw energies\n",
    "        K_energy = self.dense3d_energy(energies)\n",
    "        mbd_energy = torch.tanh(minibatch_discriminator(K_energy))\n",
    "\n",
    "        # Absolute deviation from input energy\n",
    "        energy_well = torch.abs(total_energy - input_energy)\n",
    "        \n",
    "        # Binary y/n if it is over the input energy\n",
    "        well_too_big = 10 * (energy_well > 5).float()\n",
    "\n",
    "        # Concatenate all features\n",
    "        p = torch.cat([\n",
    "            features,\n",
    "            scale(energies, 10),\n",
    "            scale(total_energy, 100),\n",
    "            energy_well,\n",
    "            well_too_big,\n",
    "            mbd_energy\n",
    "        ], dim=1)\n",
    "\n",
    "        fake = torch.sigmoid(self.fc(p))\n",
    "        discriminator_outputs = [fake, total_energy]\n",
    "\n",
    "        # Auxiliary output for ACGAN\n",
    "        if self.nb_classes > 1:\n",
    "            aux = torch.sigmoid(self.aux_fc(p))\n",
    "            discriminator_outputs.append(aux)\n",
    "\n",
    "        return discriminator_outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generatore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self, latent_size, no_attn, nb_classes=1):\n",
    "        super(Generator, self).__init__()\n",
    "        self.latent_size = latent_size\n",
    "        self.nb_classes = nb_classes\n",
    "        self.no_attn = no_attn\n",
    "\n",
    "        # Embedding layer\n",
    "        if nb_classes > 1:\n",
    "            self.embedding = nn.Embedding(nb_classes, latent_size)\n",
    "            self.flatten = nn.Flatten()\n",
    "\n",
    "\n",
    "        # Define generator layers\n",
    "        self.gen_layer0 = build_Generator(latent_size, 3, 96)\n",
    "        self.gen_layer1 = build_Generator(latent_size, 12, 12)\n",
    "        self.gen_layer2 = build_Generator(latent_size, 12, 6)\n",
    "\n",
    "        if not no_attn:\n",
    "            self.attn_layer1=InpaintingAttention(constant=-10.0, input_size=[14,14])\n",
    "            self.attn_layer2=InpaintingAttention(constant=-10.0, input_size=[14,8])\n",
    "\n",
    "\n",
    "    def forward(self, generator_inputs, image_class=None):\n",
    "        latent=generator_inputs[0]\n",
    "        input_energy=generator_inputs[1]\n",
    "        if self.nb_classes > 1 and image_class is not None:\n",
    "            emb = self.embedding(image_class)\n",
    "            emb = self.flatten(emb)\n",
    "            hc = latent * emb\n",
    "            h = hc * scale(input_energy, 100)\n",
    "        else:\n",
    "            h = latent * scale(input_energy, 100).shape[1]\n",
    "\n",
    "        img_layer0 = self.gen_layer0(h)\n",
    "        img_layer1 = self.gen_layer1(h)\n",
    "        img_layer2 = self.gen_layer2(h)\n",
    "\n",
    "        if not no_attn:\n",
    "            # resizes from (3, 96) => (12, 12)\n",
    "            zero2one = nn.AvgPool2d(kernel_size=(1, 8))(\n",
    "                nn.Upsample(scale_factor=(4, 1), mode='nearest')(img_layer0))\n",
    "            img_layer1 = self.attn_layer1(img_layer1, zero2one)\n",
    "            \n",
    "            # resizes from (12, 12) => (12, 6)\n",
    "            one2two = nn.AvgPool2d(kernel_size=(1, 2))(img_layer1)\n",
    "            img_layer2 = self.attn_layer2(img_layer2, one2two)\n",
    "     \n",
    "\n",
    "        return [F.relu(img_layer0), F.relu(img_layer1), F.relu(img_layer2)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combined model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "latent torch.Size([256, 100])\n",
      "input torch.Size([256, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dario/Desktop/CaloGAN/.venv/lib/python3.12/site-packages/torch/nn/modules/conv.py:456: UserWarning: Using padding='same' with even kernel lengths and odd dilation may require a zero-padded copy of the input be created (Triggered internally at ../aten/src/ATen/native/Convolution.cpp:1031.)\n",
      "  return F.conv2d(input, weight, bias, self.stride,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([256])\n",
      "fake torch.Size([256, 1])\n",
      "energy torch.Size([256, 1])\n"
     ]
    }
   ],
   "source": [
    "inputs = [3,96,12,12,12,6]\n",
    "discriminator = Discriminator(inputs)\n",
    "\n",
    "latent = torch.randn(batch_size, latent_size)  # Example latent input\n",
    "input_energy = torch.randn((batch_size, 1)) \n",
    "generator_inputs = [latent, input_energy] \n",
    "print(\"latent\", latent.shape)\n",
    "print(\"input\",input_energy.shape)\n",
    "\n",
    "\n",
    "generator=Generator(latent_size, False)\n",
    "out=generator(generator_inputs)\n",
    "#combined_input = [torch.cat([out[i], input_energy], dim=0) for i in range(3)]\n",
    "#print(out[0].shape)\n",
    "sampled_energies = torch.rand( size=(batch_size, 1))*99+1\n",
    "out2=discriminator(out, sampled_energies)\n",
    "print(out2[0].view(-1).shape)\n",
    "#print(out2[2].requires_grad)\n",
    "\n",
    "\n",
    "class CombinedModel(nn.Module):\n",
    "    def __init__(self, generator, discriminator):\n",
    "        super(CombinedModel, self).__init__()\n",
    "        self.generator = generator\n",
    "        self.discriminator = discriminator\n",
    "\n",
    "    def forward(self, generator_inputs, image_class=None):\n",
    "        latent=generator_inputs[0]\n",
    "        input_energy=generator_inputs[1]\n",
    "        gen_output = self.generator(generator_inputs, image_class)\n",
    "        disc_output = self.discriminator(gen_output, input_energy)\n",
    "        return disc_output\n",
    "\n",
    "# Instantiate the combined model\n",
    "\n",
    "\n",
    "combined = CombinedModel(generator, discriminator)\n",
    "out=combined(generator_inputs)\n",
    "print(\"fake\",out[0].shape)\n",
    "print(\"energy\", out[1].shape)\n",
    "#summary(combined, input_size=[(1,latent_size), (1,1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters in Generator: 3177720\n",
      "Number of parameters in Discriminator: 2735604\n",
      "Number of parameters in Combined Model: 5913324\n"
     ]
    }
   ],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "# Count parameters for each individual model\n",
    "generator_params = count_parameters(generator)\n",
    "discriminator_params = count_parameters(discriminator)\n",
    "combined_params = count_parameters(combined)\n",
    "\n",
    "print(f\"Number of parameters in Generator: {generator_params}\")\n",
    "print(f\"Number of parameters in Discriminator: {discriminator_params}\")\n",
    "print(f\"Number of parameters in Combined Model: {combined_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage of Generator: 12.12 MB\n",
      "Memory usage of Discriminator: 10.44 MB\n",
      "Memory usage of Combined Model: 22.56 MB\n"
     ]
    }
   ],
   "source": [
    "def calculate_memory_usage(model):\n",
    "    total_memory = 0\n",
    "    for param in model.parameters():\n",
    "        if param.requires_grad:\n",
    "            # The number of elements in the parameter\n",
    "            num_elements = param.numel()\n",
    "            # Size of each element in bytes (assuming float32, which is 4 bytes)\n",
    "            element_size = param.element_size()\n",
    "            # Total memory for this parameter\n",
    "            total_memory += num_elements * element_size\n",
    "    return total_memory\n",
    "\n",
    "def format_memory_size(size_in_bytes):\n",
    "    \"\"\" Convert the memory size from bytes to a readable format (KB, MB, GB) \"\"\"\n",
    "    for unit in ['B', 'KB', 'MB', 'GB', 'TB']:\n",
    "        if size_in_bytes < 1024:\n",
    "            return f\"{size_in_bytes:.2f} {unit}\"\n",
    "        size_in_bytes /= 1024\n",
    "\n",
    "# Calculate memory usage for each individual model\n",
    "generator_memory = calculate_memory_usage(generator)\n",
    "discriminator_memory = calculate_memory_usage(discriminator)\n",
    "combined_memory = calculate_memory_usage(combined)\n",
    "\n",
    "print(f\"Memory usage of Generator: {format_memory_size(generator_memory)}\")\n",
    "print(f\"Memory usage of Discriminator: {format_memory_size(discriminator_memory)}\")\n",
    "print(f\"Memory usage of Combined Model: {format_memory_size(combined_memory)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SaveBestModel:\n",
    "    def __init__(self, best_valid_loss=float('inf')): #object initialized with best_loss = +infinite\n",
    "        self.best_valid_loss = best_valid_loss\n",
    "\n",
    "    def __call__(\n",
    "        self, current_valid_loss,\n",
    "        epoch, model, optimizer, criterion, metric\n",
    "    ):\n",
    "        if current_valid_loss < self.best_valid_loss:\n",
    "            self.best_valid_loss = current_valid_loss\n",
    "\n",
    "            print(f\"\\nBest validation loss: {self.best_valid_loss}\")\n",
    "            print(f\"\\nSaving best model for epoch: {epoch+1}\\n\")\n",
    "\n",
    "            # method to save a model (the state_dict: a python dictionary object that\n",
    "            # maps each layer to its parameter tensor) and other useful parametrers\n",
    "            # see: https://pytorch.org/tutorials/beginner/saving_loading_models.html\n",
    "\n",
    "            torch.save({'model' : model,\n",
    "                'epoch': epoch+1,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'loss': criterion,'metric': metric,\n",
    "                }, 'best_model.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' if torch.backends.mps.is_available():\\n    device = torch.device(\"mps\")\\n    print(\"MPS device is available and being used\")\\nelse:\\n    device = torch.device(\"cpu\")\\n    print(\"MPS device not available, using CPU\")\\n '"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "opt_g = torch.optim.Adam(params=generator.parameters(), lr=gen_lr, weight_decay=1e-5)\n",
    "opt_d = torch.optim.Adam(params=discriminator.parameters(), lr=disc_lr, weight_decay=1e-5)\n",
    "\n",
    "bce_loss=nn.BCELoss()\n",
    "mae_loss=nn.L1Loss()\n",
    "\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "generator.to(device)\n",
    "discriminator.to(device)\n",
    "\n",
    "\n",
    "\n",
    "model=CombinedModel(generator, discriminator)\n",
    "model.to(device)\n",
    "\n",
    "disc_loss = []\n",
    "gen_loss = []\n",
    "\n",
    "\"\"\" if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "    print(\"MPS device is available and being used\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"MPS device not available, using CPU\")\n",
    " \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 0/39 [00:00<?, ?it/s]/home/dario/Desktop/CaloGAN/.venv/lib/python3.12/site-packages/torch/nn/modules/loss.py:101: UserWarning: Using a target size (torch.Size([256, 1])) that is different to the input size (torch.Size([256])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.l1_loss(input, target, reduction=self.reduction)\n",
      "Training: 100%|██████████| 39/39 [11:49<00:00, 18.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   1 Generator loss: 35.72140151262283\n",
      "Epoch   1 Discriminator loss: 14.400022314861417\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "disc_loss = []\n",
    "gen_loss = []\n",
    "    \n",
    "ones = torch.ones(batch_size,device=device)\n",
    "zeros = torch.zeros(batch_size,device=device)\n",
    "\n",
    "nb_epochs=1\n",
    "for epoch in range(nb_epochs):\n",
    "    t0 = time.time()\n",
    "    train_loss = 0\n",
    "    counter=0\n",
    "    disc_loss_partial=0\n",
    "    gen_loss_partial=0\n",
    "\n",
    "    for image_batch_1,image_batch_2, image_batch_3, energy_batch, label_batch in tqdm(dataloader, desc=\"Training\"):\n",
    "        \n",
    "        opt_d.zero_grad()\n",
    "\n",
    "        noise = torch.normal(0, 1, size=(batch_size, latent_size), device=device)\n",
    "        image_batch_1 = image_batch_1.to(device)\n",
    "        image_batch_2 = image_batch_2.to(device)\n",
    "        image_batch_3 = image_batch_3.to(device)\n",
    "        # energy_breakdown\n",
    "\n",
    "        sampled_labels = torch.randint(0, nb_classes, size=(batch_size,),device=device)\n",
    "        sampled_energies = torch.rand( size=(batch_size, 1),device=device)*99+1\n",
    "\n",
    "        generator_inputs = [noise, sampled_energies]\n",
    "\n",
    "        \"\"\" if nb_classes > 1:\n",
    "                # in the case of the ACGAN, we need to append the requested\n",
    "                # class to the pre-image of the generator\n",
    "                generator_inputs.append(sampled_labels) \"\"\"\n",
    "\n",
    "        generated_images = generator(generator_inputs)\n",
    "\n",
    "        #disc_outputs_real = [torch.ones(batch_size), energy_batch]\n",
    "        #disc_outputs_fake = [torch.zeros(batch_size), sampled_energies]\n",
    "\n",
    "        #loss_weights = torch.Tensor([1, 0.05]\n",
    "        #print(loss_weights.requires_grad)\n",
    "\n",
    "        \n",
    "        \n",
    "        out = discriminator([image_batch_1,image_batch_2, image_batch_3], energy_batch)\n",
    "        loss_real =   bce_loss(out[0].view(-1), ones) + 0.05 * mae_loss(out[1].view(-1), energy_batch)\n",
    "        \n",
    "        loss_real.backward()\n",
    "\n",
    "        out = discriminator(generated_images, sampled_energies)\n",
    "        loss_fake =  bce_loss(out[0].view(-1), zeros) + 0.05*mae_loss(out[1].view(-1), sampled_energies)\n",
    "        \n",
    "        loss_fake.backward()\n",
    "        opt_d.step()\n",
    "\n",
    "        disc_loss_partial+=((loss_real.item() + loss_fake.item()) / 2)\n",
    "        \n",
    "        opt_g.zero_grad()\n",
    "\n",
    "\n",
    "        noise = torch.normal(0, 1, size=(batch_size, latent_size), device=device)\n",
    "        sampled_energies = torch.rand( size=(batch_size, 1),device=device)*99+1\n",
    "        combined_inputs = [noise, sampled_energies]\n",
    "        out=discriminator(generator(combined_inputs),sampled_energies)\n",
    "\n",
    "        loss_gen=bce_loss(out[0].view(-1), ones) + 0.05*mae_loss(out[1].view(-1), sampled_energies)\n",
    "        loss_gen.backward()\n",
    "\n",
    "        opt_g.step()\n",
    "        gen_loss_partial+=loss_gen.item()\n",
    "\n",
    "\n",
    "    disc_loss.append(disc_loss_partial/batch_size)\n",
    "    gen_loss.append(gen_loss_partial/batch_size)\n",
    "    print('Epoch {:3d} Generator loss: {}'.format(epoch + 1, gen_loss_partial/batch_size))\n",
    "    print(('Epoch {:3d} Discriminator loss: {}'.format(epoch + 1, disc_loss_partial/batch_size)))\n",
    "        \n",
    "    \n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
