{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import argparse\n",
    "from collections import defaultdict\n",
    "import logging\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.utils import shuffle\n",
    "import sys\n",
    "import yaml\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch.optim as optim\n",
    "import h5py\n",
    "import logging\n",
    "import sys\n",
    "from tqdm import tqdm\n",
    "\n",
    "from ops_pytorch import (minibatch_discriminator, minibatch_output_shape, Dense3D,\n",
    "                     calculate_energy, scale, InpaintingAttention)\n",
    "\n",
    "from architectures_torch import build_Generator, build_Discriminator\n",
    "\n",
    "import time\n",
    "\n",
    "from torchsummary import summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Settaggio Parametri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "[['positron', '../data/eplus.hdf5']]\n"
     ]
    }
   ],
   "source": [
    "nb_epochs = 50\n",
    "batch_size = 256\n",
    "latent_size = 100\n",
    "verbose = True\n",
    "no_attn = True\n",
    "\n",
    "disc_lr =2e-5\n",
    "gen_lr = 2e-4\n",
    "adam_beta_1 = 0.5\n",
    "\n",
    "yaml_file = \"particles.yaml\"\n",
    "\n",
    "    # read in data file spec from YAML file\n",
    "with open(yaml_file, 'r') as stream:\n",
    "    try:\n",
    "        s = yaml.safe_load(stream)\n",
    "    except yaml.YAMLError as exc:\n",
    "\n",
    "        raise exc\n",
    "nb_classes = len(s.keys())\n",
    "print(nb_classes)\n",
    "a=[[p,f] for p,f in s.items()]\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Caricamento dati "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bit_flip(x, prob=0.05):\n",
    "    \"\"\" flips a int array's values with some probability \"\"\"\n",
    "    x = np.array(x)\n",
    "    selection = np.random.uniform(0, 1, x.shape) < prob\n",
    "    x[selection] = 1 * np.logical_not(x[selection])\n",
    "    return x\n",
    "\n",
    "def _load_data(particle, datafile):\n",
    "\n",
    "\n",
    "        d = h5py.File(datafile, 'r')\n",
    "\n",
    "        # make our calo images channels-last\n",
    "        first = np.expand_dims(d['layer_0'][:], -1)\n",
    "        second = np.expand_dims(d['layer_1'][:], -1)\n",
    "        third = np.expand_dims(d['layer_2'][:], -1)\n",
    "        # convert to MeV\n",
    "        energy = d['energy'][:].reshape(-1, 1) * 1000\n",
    "\n",
    "        sizes = [\n",
    "            first.shape[1], first.shape[2],\n",
    "            second.shape[1], second.shape[2],\n",
    "            third.shape[1], third.shape[2]\n",
    "        ]\n",
    "\n",
    "        y = [particle] * first.shape[0]\n",
    "\n",
    "        d.close()\n",
    "\n",
    "        return first, second, third, y, energy, sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([256, 1, 3, 96])\n",
      "torch.Size([256, 1, 12, 12])\n",
      "torch.Size([256, 1, 12, 6])\n",
      "torch.Size([256, 1])\n",
      "torch.Size([256])\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n"
     ]
    }
   ],
   "source": [
    "first, second, third, y, energy, sizes = [\n",
    "    np.concatenate(t) for t in [\n",
    "        a for a in zip(*[_load_data(p, f) for p, f in s.items()])\n",
    "    ]\n",
    "]\n",
    "\n",
    "sizes = sizes[:6].tolist()\n",
    "\n",
    "# scale the energy depositions by 1000 to convert MeV => GeV\n",
    "first, second, third, energy = [\n",
    "    (X.astype(np.float32) / 1000)\n",
    "    for X in [first, second, third, energy]\n",
    "    ]\n",
    "le = LabelEncoder()\n",
    "y = le.fit_transform(y)\n",
    "\n",
    "first = torch.tensor(first, dtype=torch.float32)[:10000].permute(0, 3, 1, 2)\n",
    "second = torch.tensor(second, dtype=torch.float32)[:10000].permute(0, 3, 1, 2)\n",
    "third = torch.tensor(third, dtype=torch.float32)[:10000].permute(0, 3, 1, 2)\n",
    "y = torch.tensor(y, dtype=torch.long)[:10000]\n",
    "energy = torch.tensor(energy, dtype=torch.float32, requires_grad=True)[:10000]\n",
    "# Dataset and DataLoader\n",
    "dataset = TensorDataset(first, second, third, energy,y)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "x=next(iter(dataloader))\n",
    "print(x[0].shape)\n",
    "print(x[1].shape)\n",
    "print(x[2].shape)\n",
    "print(x[3].shape)\n",
    "print(x[4].shape)\n",
    "print(x[4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discriminatore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, sizes, nb_classes=1):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.nb_classes = nb_classes\n",
    "        self.mbd = True\n",
    "        self.sparsity = True\n",
    "        self.sparsity_mbd = True\n",
    "\n",
    "        self.layers = nn.ModuleList([\n",
    "            build_Discriminator(mbd=self.mbd, sparsity=self.sparsity, sparsity_mbd=self.sparsity_mbd,sizes=sizes[2*i:(i+1)*2])\n",
    "            for i in range(3)\n",
    "        ])\n",
    "\n",
    "#NOTE: con tutto true il fc ha bisogno di 1863, mentre nel caso false solo 1800\n",
    "        self.fc = nn.Linear(1863, 1) \n",
    "        self.aux_fc = nn.Linear(1863, 1) if nb_classes > 1 else None\n",
    "\n",
    "        # For minibatch discrimination\n",
    "        self.dense3d_energy = Dense3D(first_dim=10, last_dim=10, input_shape=(3,1))\n",
    "\n",
    "    def forward(self, inputs, input_energy):\n",
    "        features = []\n",
    "        energies = []\n",
    "\n",
    "        # Extract features and energies from each calorimeter layer\n",
    "        for i, input in enumerate(inputs):\n",
    "            features.append(self.layers[i](input))\n",
    "            energies.append(calculate_energy(input))\n",
    "\n",
    "        # Concatenate features\n",
    "        features = torch.cat(features, dim=1)\n",
    "        energies = torch.cat(energies,dim=1)\n",
    "\n",
    "        # Total energy across all rows\n",
    "        total_energy = torch.sum(energies, dim=1, keepdim=True)\n",
    "\n",
    "        # Minibatch discrimination on the raw energies\n",
    "        K_energy = self.dense3d_energy(energies)\n",
    "        mbd_energy = torch.tanh(minibatch_discriminator(K_energy))\n",
    "\n",
    "        # Absolute deviation from input energy\n",
    "        energy_well = torch.abs(total_energy - input_energy)\n",
    "        \n",
    "        # Binary y/n if it is over the input energy\n",
    "        well_too_big = 10 * (energy_well > 5).float()\n",
    "\n",
    "        # Concatenate all features\n",
    "        p = torch.cat([\n",
    "            features,\n",
    "            scale(energies, 10),\n",
    "            scale(total_energy, 100),\n",
    "            energy_well,\n",
    "            well_too_big,\n",
    "            mbd_energy\n",
    "        ], dim=1)\n",
    "\n",
    "        fake = torch.sigmoid(self.fc(p))\n",
    "        discriminator_outputs = [fake, total_energy]\n",
    "\n",
    "        # Auxiliary output for ACGAN\n",
    "        if self.nb_classes > 1:\n",
    "            aux = torch.sigmoid(self.aux_fc(p))\n",
    "            discriminator_outputs.append(aux)\n",
    "\n",
    "        return discriminator_outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generatore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self, latent_size, no_attn, nb_classes=1):\n",
    "        super(Generator, self).__init__()\n",
    "        self.latent_size = latent_size\n",
    "        self.nb_classes = nb_classes\n",
    "        self.no_attn = no_attn\n",
    "\n",
    "        # Embedding layer\n",
    "        if nb_classes > 1:\n",
    "            self.embedding = nn.Embedding(nb_classes, latent_size)\n",
    "            self.flatten = nn.Flatten()\n",
    "\n",
    "\n",
    "        # Define generator layers\n",
    "        self.gen_layer0 = build_Generator(latent_size, 3, 96)\n",
    "        self.gen_layer1 = build_Generator(latent_size, 12, 12)\n",
    "        self.gen_layer2 = build_Generator(latent_size, 12, 6)\n",
    "\n",
    "        if not no_attn:\n",
    "            self.attn_layer1=InpaintingAttention(constant=-10.0, input_size=[14,14])\n",
    "            self.attn_layer2=InpaintingAttention(constant=-10.0, input_size=[14,8])\n",
    "\n",
    "\n",
    "    def forward(self, generator_inputs, image_class=None):\n",
    "        latent=generator_inputs[0]\n",
    "        input_energy=generator_inputs[1]\n",
    "        if self.nb_classes > 1 and image_class is not None:\n",
    "            emb = self.embedding(image_class)\n",
    "            emb = self.flatten(emb)\n",
    "            hc = latent * emb\n",
    "            h = hc * scale(input_energy, 100)\n",
    "        else:\n",
    "            h = latent * scale(input_energy, 100).shape[1]\n",
    "\n",
    "        img_layer0 = self.gen_layer0(h)\n",
    "        img_layer1 = self.gen_layer1(h)\n",
    "        img_layer2 = self.gen_layer2(h)\n",
    "\n",
    "        if not no_attn:\n",
    "            # resizes from (3, 96) => (12, 12)\n",
    "            zero2one = nn.AvgPool2d(kernel_size=(1, 8))(\n",
    "                nn.Upsample(scale_factor=(4, 1), mode='nearest')(img_layer0))\n",
    "            img_layer1 = self.attn_layer1(img_layer1, zero2one)\n",
    "            \n",
    "            # resizes from (12, 12) => (12, 6)\n",
    "            one2two = nn.AvgPool2d(kernel_size=(1, 2))(img_layer1)\n",
    "            img_layer2 = self.attn_layer2(img_layer2, one2two)\n",
    "     \n",
    "\n",
    "        return [F.relu(img_layer0), F.relu(img_layer1), F.relu(img_layer2)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combined model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "latent torch.Size([256, 100])\n",
      "input torch.Size([256, 1])\n",
      "torch.Size([256])\n",
      "True\n",
      "fake torch.Size([256, 1])\n",
      "energy torch.Size([256, 1])\n"
     ]
    }
   ],
   "source": [
    "inputs = [3,96,12,12,12,6]\n",
    "discriminator = Discriminator(inputs,2)\n",
    "\n",
    "\n",
    "for param in discriminator.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "latent = torch.randn(batch_size, latent_size)  # Example latent input\n",
    "input_energy = torch.randn((batch_size, 1)) \n",
    "generator_inputs = [latent, input_energy] \n",
    "print(\"latent\", latent.shape)\n",
    "print(\"input\",input_energy.shape)\n",
    "\n",
    "\n",
    "generator=Generator(latent_size, False)\n",
    "out=generator(generator_inputs)\n",
    "#combined_input = [torch.cat([out[i], input_energy], dim=0) for i in range(3)]\n",
    "#print(out[0].shape)\n",
    "sampled_energies = torch.rand( size=(batch_size, 1))*99+1\n",
    "out2=discriminator(out, sampled_energies)\n",
    "print(out2[0].view(-1).shape)\n",
    "print(out2[2].requires_grad)\n",
    "\n",
    "\n",
    "class CombinedModel(nn.Module):\n",
    "    def __init__(self, generator, discriminator):\n",
    "        super(CombinedModel, self).__init__()\n",
    "        self.generator = generator\n",
    "        self.discriminator = discriminator\n",
    "\n",
    "    def forward(self, generator_inputs, image_class=None):\n",
    "        latent=generator_inputs[0]\n",
    "        input_energy=generator_inputs[1]\n",
    "        gen_output = self.generator(generator_inputs, image_class)\n",
    "        disc_output = self.discriminator(gen_output, input_energy)\n",
    "        return disc_output\n",
    "\n",
    "# Instantiate the combined model\n",
    "\n",
    "\n",
    "combined = CombinedModel(generator, discriminator)\n",
    "out=combined(generator_inputs)\n",
    "print(\"fake\",out[0].shape)\n",
    "print(\"energy\", out[1].shape)\n",
    "#summary(combined, input_size=[(1,latent_size), (1,1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters in Generator: 3177720\n",
      "Number of parameters in Discriminator: 0\n",
      "Number of parameters in Combined Model: 3177720\n"
     ]
    }
   ],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "# Count parameters for each individual model\n",
    "generator_params = count_parameters(generator)\n",
    "discriminator_params = count_parameters(discriminator)\n",
    "combined_params = count_parameters(combined)\n",
    "\n",
    "print(f\"Number of parameters in Generator: {generator_params}\")\n",
    "print(f\"Number of parameters in Discriminator: {discriminator_params}\")\n",
    "print(f\"Number of parameters in Combined Model: {combined_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage of Generator: 12.12 MB\n",
      "Memory usage of Discriminator: 0.00 B\n",
      "Memory usage of Combined Model: 12.12 MB\n"
     ]
    }
   ],
   "source": [
    "def calculate_memory_usage(model):\n",
    "    total_memory = 0\n",
    "    for param in model.parameters():\n",
    "        if param.requires_grad:\n",
    "            # The number of elements in the parameter\n",
    "            num_elements = param.numel()\n",
    "            # Size of each element in bytes (assuming float32, which is 4 bytes)\n",
    "            element_size = param.element_size()\n",
    "            # Total memory for this parameter\n",
    "            total_memory += num_elements * element_size\n",
    "    return total_memory\n",
    "\n",
    "def format_memory_size(size_in_bytes):\n",
    "    \"\"\" Convert the memory size from bytes to a readable format (KB, MB, GB) \"\"\"\n",
    "    for unit in ['B', 'KB', 'MB', 'GB', 'TB']:\n",
    "        if size_in_bytes < 1024:\n",
    "            return f\"{size_in_bytes:.2f} {unit}\"\n",
    "        size_in_bytes /= 1024\n",
    "\n",
    "# Calculate memory usage for each individual model\n",
    "generator_memory = calculate_memory_usage(generator)\n",
    "discriminator_memory = calculate_memory_usage(discriminator)\n",
    "combined_memory = calculate_memory_usage(combined)\n",
    "\n",
    "print(f\"Memory usage of Generator: {format_memory_size(generator_memory)}\")\n",
    "print(f\"Memory usage of Discriminator: {format_memory_size(discriminator_memory)}\")\n",
    "print(f\"Memory usage of Combined Model: {format_memory_size(combined_memory)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SaveBestModel:\n",
    "    def __init__(self, best_valid_loss=float('inf')): #object initialized with best_loss = +infinite\n",
    "        self.best_valid_loss = best_valid_loss\n",
    "\n",
    "    def __call__(\n",
    "        self, current_valid_loss,\n",
    "        epoch, model, optimizer, criterion, metric\n",
    "    ):\n",
    "        if current_valid_loss < self.best_valid_loss:\n",
    "            self.best_valid_loss = current_valid_loss\n",
    "\n",
    "            print(f\"\\nBest validation loss: {self.best_valid_loss}\")\n",
    "            print(f\"\\nSaving best model for epoch: {epoch+1}\\n\")\n",
    "\n",
    "            # method to save a model (the state_dict: a python dictionary object that\n",
    "            # maps each layer to its parameter tensor) and other useful parametrers\n",
    "            # see: https://pytorch.org/tutorials/beginner/saving_loading_models.html\n",
    "\n",
    "            torch.save({'model' : model,\n",
    "                'epoch': epoch+1,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'loss': criterion,'metric': metric,\n",
    "                }, 'best_model.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' if torch.backends.mps.is_available():\\n    device = torch.device(\"mps\")\\n    print(\"MPS device is available and being used\")\\nelse:\\n    device = torch.device(\"cpu\")\\n    print(\"MPS device not available, using CPU\")\\n '"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "opt_g = torch.optim.Adam(params=generator.parameters(), lr=gen_lr, weight_decay=1e-5)\n",
    "opt_d = torch.optim.Adam(params=discriminator.parameters(), lr=disc_lr, weight_decay=1e-5)\n",
    "\n",
    "bce_loss=nn.BCELoss()\n",
    "mae_loss=nn.L1Loss()\n",
    "\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "generator.to(device)\n",
    "discriminator.to(device)\n",
    "\n",
    "\n",
    "\n",
    "model=CombinedModel(generator, discriminator)\n",
    "model.to(device)\n",
    "\n",
    "disc_loss = []\n",
    "gen_loss = []\n",
    "\n",
    "\"\"\" if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "    print(\"MPS device is available and being used\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"MPS device not available, using CPU\")\n",
    " \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 0/39 [00:00<?, ?it/s]/Users/martinachirico/miniconda3/lib/python3.10/site-packages/torch/nn/modules/loss.py:101: UserWarning: Using a target size (torch.Size([256, 1])) that is different to the input size (torch.Size([256])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.l1_loss(input, target, reduction=self.reduction)\n",
      "Training:   5%|▌         | 2/39 [00:09<03:00,  4.87s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 66\u001b[0m\n\u001b[1;32m     63\u001b[0m                 combined_inputs\u001b[38;5;241m.\u001b[39mappend(sampled_labels)\n\u001b[1;32m     64\u001b[0m                 combined_outputs\u001b[38;5;241m.\u001b[39mappend(sampled_labels)\n\u001b[0;32m---> 66\u001b[0m             out\u001b[38;5;241m=\u001b[39m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcombined_inputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     67\u001b[0m             gen_loss_partial\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39mbce_loss(out[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m), trick) \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m0.05\u001b[39m\u001b[38;5;241m*\u001b[39mmae_loss(out[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m), sampled_energies)\n\u001b[1;32m     69\u001b[0m disc_loss\u001b[38;5;241m.\u001b[39mappend(disc_loss_partial\u001b[38;5;241m/\u001b[39mbatch_size)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[15], line 35\u001b[0m, in \u001b[0;36mCombinedModel.forward\u001b[0;34m(self, generator_inputs, image_class)\u001b[0m\n\u001b[1;32m     33\u001b[0m input_energy\u001b[38;5;241m=\u001b[39mgenerator_inputs[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m     34\u001b[0m gen_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgenerator(generator_inputs, image_class)\n\u001b[0;32m---> 35\u001b[0m disc_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdiscriminator\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgen_output\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_energy\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m disc_output\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[13], line 27\u001b[0m, in \u001b[0;36mDiscriminator.forward\u001b[0;34m(self, inputs, input_energy)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m# Extract features and energies from each calorimeter layer\u001b[39;00m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, \u001b[38;5;28minput\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(inputs):\n\u001b[0;32m---> 27\u001b[0m     features\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayers\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     28\u001b[0m     energies\u001b[38;5;241m.\u001b[39mappend(calculate_energy(\u001b[38;5;28minput\u001b[39m))\n\u001b[1;32m     30\u001b[0m \u001b[38;5;66;03m# Concatenate features\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Desktop/CaloGAN/models/architectures_torch.py:108\u001b[0m, in \u001b[0;36mbuild_Discriminator.forward\u001b[0;34m(self, image)\u001b[0m\n\u001b[1;32m    106\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmbd:\n\u001b[1;32m    107\u001b[0m     K_x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdense3d_1(x)\n\u001b[0;32m--> 108\u001b[0m     features\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactivation_tanh(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mminibatch_featurizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mK_x\u001b[49m\u001b[43m)\u001b[49m))\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msparsity \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msparsity_mbd:\n\u001b[1;32m    112\u001b[0m     empirical_sparsity \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msparsity_detector(image)\n",
      "File \u001b[0;32m~/Desktop/CaloGAN/models/ops_pytorch.py:156\u001b[0m, in \u001b[0;36mminibatch_discriminator\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mminibatch_discriminator\u001b[39m(x):\n\u001b[1;32m    155\u001b[0m     \u001b[38;5;66;03m# Expand dimensions and compute differences\u001b[39;00m\n\u001b[0;32m--> 156\u001b[0m     diffs \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m3\u001b[39m) \u001b[38;5;241m-\u001b[39m \u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpermute\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munsqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    158\u001b[0m     \u001b[38;5;66;03m# Compute the L1 norm\u001b[39;00m\n\u001b[1;32m    159\u001b[0m     l1_norm \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msum(torch\u001b[38;5;241m.\u001b[39mabs(diffs), dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "nb_epochs=1\n",
    "for epoch in range(nb_epochs):\n",
    "    t0 = time.time()\n",
    "    train_loss = 0\n",
    "    counter=0\n",
    "    disc_loss_partial=0\n",
    "    gen_loss_partial=0\n",
    "    for image_batch_1,image_batch_2, image_batch_3, energy_batch, label_batch in tqdm(dataloader, desc=\"Training\"):\n",
    "        time.sleep(0.1)\n",
    "\n",
    "        noise = torch.normal(0, 1, size=(batch_size, latent_size), requires_grad=True)\n",
    "\n",
    "        # energy_breakdown\n",
    "\n",
    "        sampled_labels = torch.randint(0, nb_classes, size=(batch_size,))\n",
    "        sampled_energies = torch.rand( size=(batch_size, 1))*99+1\n",
    "\n",
    "        generator_inputs = [noise, sampled_energies]\n",
    "\n",
    "        \"\"\" if nb_classes > 1:\n",
    "                # in the case of the ACGAN, we need to append the requested\n",
    "                # class to the pre-image of the generator\n",
    "                generator_inputs.append(sampled_labels) \"\"\"\n",
    "\n",
    "        generated_images = generator(generator_inputs)\n",
    "\n",
    "        #disc_outputs_real = [torch.ones(batch_size), energy_batch]\n",
    "        #disc_outputs_fake = [torch.zeros(batch_size), sampled_energies]\n",
    "\n",
    "        #loss_weights = torch.Tensor([1, 0.05]\n",
    "        #print(loss_weights.requires_grad)\n",
    "\n",
    "        \n",
    "        \n",
    "        out = discriminator([image_batch_1,image_batch_2, image_batch_3], energy_batch)\n",
    "        loss_real =   bce_loss(out[0].view(-1), torch.ones(batch_size)) + 0.05 * mae_loss(out[1].view(-1), energy_batch)\n",
    "        \n",
    "        opt_d.zero_grad()\n",
    "        loss_real.backward()\n",
    "        opt_d.step()\n",
    "\n",
    "        out = discriminator(generated_images, sampled_energies)\n",
    "        loss_fake =  bce_loss(out[0].view(-1), torch.zeros(batch_size)) + 0.05*mae_loss(out[1].view(-1), sampled_energies)\n",
    "        \n",
    "        opt_d.zero_grad()\n",
    "        loss_fake.backward()\n",
    "        opt_d.step()\n",
    "\n",
    "        disc_loss_partial+=((loss_real.item() + loss_fake.item()) / 2)\n",
    "        \n",
    "\n",
    "        trick = torch.ones(batch_size)\n",
    "\n",
    "        for _ in range(2):\n",
    "            noise = torch.normal(0, 1, size=(batch_size, latent_size), requires_grad=True)\n",
    "\n",
    "            sampled_energies = torch.rand( size=(batch_size, 1))*99+1\n",
    "            combined_inputs = [noise, sampled_energies]\n",
    "            combined_outputs = [trick, sampled_energies]\n",
    "            if nb_classes > 1:\n",
    "                sampled_labels = np.random.randint(0, nb_classes,\n",
    "                                                   batch_size)\n",
    "                combined_inputs.append(sampled_labels)\n",
    "                combined_outputs.append(sampled_labels)\n",
    "\n",
    "            out=model(combined_inputs)\n",
    "            gen_loss_partial+=bce_loss(out[0].view(-1), trick) + 0.05*mae_loss(out[1].view(-1), sampled_energies)\n",
    "\n",
    "disc_loss.append(disc_loss_partial/batch_size)\n",
    "gen_loss.append(gen_loss_partial/batch_size)\n",
    "print('Epoch {:3d} Generator loss: {}'.format(epoch + 1, gen_loss_partial/batch_size))\n",
    "print(('Epoch {:3d} Discriminator loss: {}'.format(epoch + 1, disc_loss_partial/batch_size)))\n",
    "        \n",
    "       \n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "        \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
