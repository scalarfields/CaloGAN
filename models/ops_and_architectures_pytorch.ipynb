{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn, optim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ":math:`\\text{Softmax}(x_{i}) = \\frac{\\exp(x_i)}{\\sum_j \\exp(x_j)}`\n",
    "\n",
    "\n",
    "NumPy Arrays: NumPy arrays can be passed directly to TensorFlow/Keras functions. They will be automatically converted to TensorFlow tensors.\n",
    "Automatic Differentiation: TensorFlow automatically tracks operations performed on tensors and provides automatic differentiation for computing gradients.\n",
    "\n",
    "\n",
    "PyTorch: Eager execution by default. Operations are computed immediately, and the computational graph is dynamic.\n",
    "Keras with TensorFlow 2.x: Eager execution is the default mode. However, it can also operate in graph mode by using tf.function to create a static computational graph if needed.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pytorch "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[1., 1., 1.],\n",
      "         [1., 1., 1.],\n",
      "         [1., 1., 1.]],\n",
      "\n",
      "        [[1., 1., 1.],\n",
      "         [1., 1., 1.],\n",
      "         [1., 1., 1.]],\n",
      "\n",
      "        [[1., 1., 1.],\n",
      "         [1., 1., 1.],\n",
      "         [1., 1., 1.]]])\n"
     ]
    }
   ],
   "source": [
    "float_tensor = torch.zeros(3, 3, 3, dtype=torch.float32) + 1\n",
    "print(float_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[4., 5.],\n",
      "        [5., 7.],\n",
      "        [6., 7.]])\n",
      "tensor([[[0.3333, 0.3333, 0.3333],\n",
      "         [0.3333, 0.3333, 0.3333],\n",
      "         [0.3333, 0.3333, 0.3333]],\n",
      "\n",
      "        [[0.3333, 0.3333, 0.3333],\n",
      "         [0.3333, 0.3333, 0.3333],\n",
      "         [0.3333, 0.3333, 0.3333]],\n",
      "\n",
      "        [[0.3333, 0.3333, 0.3333],\n",
      "         [0.3333, 0.3333, 0.3333],\n",
      "         [0.3333, 0.3333, 0.3333]]])\n",
      "tensor([0.0900, 0.2447, 0.6652])\n"
     ]
    }
   ],
   "source": [
    "def channel_softmax(x):\n",
    "    e = torch.exp(x - torch.max(x, axis=-1, keepdims=True)[0])\n",
    "    s = torch.sum(e, axis=-1, keepdims=True)\n",
    "    return e / s\n",
    "\n",
    "x=torch.Tensor([[[3,3],[4,5]], [[4,4],[5,7]], [[5,5], [6,7]]])\n",
    "print(x[:,1])\n",
    "print(channel_softmax(float_tensor))\n",
    "print(channel_softmax(torch.Tensor([4,5,6])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0.3333, 0.3333, 0.3333],\n",
      "         [0.3333, 0.3333, 0.3333],\n",
      "         [0.3333, 0.3333, 0.3333]],\n",
      "\n",
      "        [[0.3333, 0.3333, 0.3333],\n",
      "         [0.3333, 0.3333, 0.3333],\n",
      "         [0.3333, 0.3333, 0.3333]],\n",
      "\n",
      "        [[0.3333, 0.3333, 0.3333],\n",
      "         [0.3333, 0.3333, 0.3333],\n",
      "         [0.3333, 0.3333, 0.3333]]])\n"
     ]
    }
   ],
   "source": [
    "channel_softmax = nn.Softmax(dim=-1)\n",
    "output = channel_softmax(float_tensor)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale(x, v):\n",
    "    return x / v\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Locally connected layer + Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x torch.Size([5, 3, 96, 3])\n",
      "bias torch.Size([1, 2, 32, 2])\n",
      "check here out torch.Size([2, 32, 2])\n",
      "torch.Size([16, 1, 96, 96])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/martinachirico/miniconda3/lib/python3.10/site-packages/torch/nn/modules/conv.py:459: UserWarning: Using padding='same' with even kernel lengths and odd dilation may require a zero-padded copy of the input be created (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/native/Convolution.cpp:1004.)\n",
      "  return F.conv2d(input, weight, bias, self.stride,\n"
     ]
    }
   ],
   "source": [
    "from torch.nn.modules.utils import _pair\n",
    "\n",
    "\"\"\" I modified locally connected2d so that bias is a float variable: if it is set to zero no bias is introduced, if it is set to \n",
    "any other number, that will be the bias\n",
    "I also made another change: provide the input size together with all the other parameters of the convolution. It will authomatically \n",
    "compute the output dimension and prepare all the weights.\n",
    "For now I assume that kernel size, padding, and stride are the same in both h and w dimensions \n",
    "In order to stack many of those layers I also return the output shape, which will be the input shape of the next layer\n",
    "\"\"\"\n",
    "\n",
    "\"\"\" #filters = #out_channels \"\"\"\n",
    "\n",
    "class LocallyConnected2d(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, input_size, kernel_size, bias, stride=1, padding=0, dilation=1):\n",
    "        super(LocallyConnected2d, self).__init__()\n",
    "        self.kernel_size = _pair(kernel_size)\n",
    "        self.stride = _pair(stride)\n",
    "        output_size_0=int(1+ (input_size[0] + 2*padding - dilation*(kernel_size-1)-1)/self.stride[0])\n",
    "        output_size_1=int(1+ (input_size[1] + 2*padding - dilation*(kernel_size-1)-1)/self.stride[1])\n",
    "        self.weight = nn.Parameter(\n",
    "            torch.randn(1, out_channels, in_channels, output_size_0, output_size_1, kernel_size**2)\n",
    "        )\n",
    "        if bias:\n",
    "            self.bias = nn.Parameter(\n",
    "                torch.full((1, out_channels, output_size_0, output_size_1), bias, requires_grad=True)\n",
    "            )\n",
    "        else:\n",
    "            self.register_parameter('bias', None)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        _, c, h, w = x.size()\n",
    "        kh, kw = self.kernel_size\n",
    "        dh, dw = self.stride\n",
    "        x = x.unfold(2, kh, dh).unfold(3, kw, dw)\n",
    "        x = x.contiguous().view(*x.size()[:-2], -1)\n",
    "        # Sum in in_channel and kernel_size dims       \n",
    "        out = (x.unsqueeze(1) * self.weight).sum([2, -1])\n",
    "\n",
    "        if self.bias is not None:\n",
    "            out += self.bias \n",
    "        return out\n",
    "    \n",
    "        \n",
    "   \n",
    "batch_size = 5\n",
    "in_channels = 3\n",
    "h, w = 96,3\n",
    "x = torch.randn(batch_size, in_channels, h, w)\n",
    "print(\"x\", x.shape)\n",
    "\n",
    "# Create layer and test if backpropagation works\n",
    "\n",
    "out_channels = 2\n",
    "kernel_size = 2\n",
    "stride = (3,1)\n",
    "\n",
    "\n",
    "conv = LocallyConnected2d(\n",
    "    in_channels, out_channels, [h,w], kernel_size, bias=6.0, stride=stride)\n",
    "print(\"bias\", conv.bias.shape)\n",
    "out = conv(x)\n",
    "print(\"check here\", \"out\", out[0].shape)\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, latent_dim, nb_rows, nb_cols):\n",
    "        super(Generator, self).__init__()\n",
    "        self.fc = nn.Linear(latent_dim, (nb_rows + 2) * (nb_cols + 2) * 36)\n",
    "        self.reshape_size = (nb_rows + 2, nb_cols + 2, 36)\n",
    "\n",
    "        self.conv1 = nn.Conv2d(36, 16, kernel_size=2,padding='same')\n",
    "        self.lrelu = nn.LeakyReLU()\n",
    "        self.batch_norm = nn.BatchNorm2d(16)\n",
    "\n",
    "        # Using custom LocallyConnected2d layers\n",
    "        self.local_conv1 = LocallyConnected2d(16, 6, [nb_rows+2,nb_cols+2], kernel_size=2, bias=0)\n",
    "        self.local_conv2 = LocallyConnected2d(6, 1, [nb_rows+1, nb_cols+1], kernel_size=2, bias=0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc(x)\n",
    "        x = x.view(-1, *self.reshape_size)  # (batch_size, nb_rows+2, nb_cols+2, 36)\n",
    "        \n",
    "        x = x.permute(0, 3, 1, 2)  # (batch_size, 36, nb_rows+2, nb_cols+2)\n",
    "        x = self.conv1(x)\n",
    "        x = self.lrelu(x)\n",
    "        x = self.batch_norm(x)\n",
    "        x = self.local_conv1(x)\n",
    "        \n",
    "        x = self.lrelu(x)\n",
    "    \n",
    "        x = self.local_conv2(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "latent_dim = 100\n",
    "nb_rows = 96\n",
    "nb_cols = 96\n",
    "model = Generator(latent_dim, nb_rows, nb_cols)\n",
    "\n",
    "# Creare un input fittizio\n",
    "dimensione_batch = 16\n",
    "rumore = torch.randn(dimensione_batch, latent_dim)  # Batch di 16 vettori con dimensione latent_dim\n",
    "\n",
    "\n",
    "# Generare immagini fake\n",
    "immagini_fake = model(rumore)\n",
    "print(immagini_fake.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inpainting attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" in InpaintingAttention ho tolto la funzione bias initializer perch√© mi sembra di averla implementata per bene  dentro il locally connected,\n",
    "ho aggiunto come argomento della classe InpaintingAttention la dimensione di input\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0->1 torch.Size([16, 1, 12, 12]) torch.Size([16, 1, 12, 12])\n",
      "1-2 torch.Size([16, 1, 12, 6]) torch.Size([16, 1, 12, 6])\n"
     ]
    }
   ],
   "source": [
    "\"\"\"NOTA IMPORTANTE: per keras l'ultima dimensione √® il canale, per pytorch il canale √® la seconda dimensione\"\"\"\n",
    "class InpaintingAttention(nn.Module):\n",
    "    def __init__(self, constant=-10, input_size=[14,26]):\n",
    "        super(InpaintingAttention, self).__init__()\n",
    "\n",
    "        # Define zero padding layer\n",
    "        self.pad = nn.ZeroPad2d((1, 1, 1, 1))\n",
    "        \n",
    "\n",
    "        # Define locally connected layer\n",
    "        self.lcn = LocallyConnected2d(in_channels=2, out_channels=2, input_size=input_size, kernel_size=3, bias=constant, stride=1)\n",
    "\n",
    "    def forward(self, primary, carryover):\n",
    "        # Concatenate primary and carryover along the last dimension\n",
    "        x = torch.cat((primary, carryover), dim=1)  #concatenate along channel dimension\n",
    "        \n",
    "        # Apply zero padding\n",
    "        h = self.pad(x)\n",
    "        \n",
    "        # Apply locally connected layer\n",
    "        h = self.lcn(h)\n",
    "\n",
    "        # Compute weights using channel softmax\n",
    "        weights = channel_softmax(h)\n",
    "        \n",
    "        # Compute the weighted sum\n",
    "        weighted_sum = torch.sum(x * weights, dim=1, keepdim=True)\n",
    "\n",
    "        return weighted_sum\n",
    "           \n",
    "\n",
    "model1=Generator(latent_dim, 3,96)\n",
    "model2=Generator(latent_dim, 12,12)\n",
    "model3=Generator(latent_dim, 12,6)\n",
    "img_layer0 = model1(rumore)\n",
    "img_layer1 = model2(rumore)\n",
    "img_layer2 = model3(rumore)\n",
    "no_attn=0\n",
    "\n",
    "inpainting_attention1=InpaintingAttention(constant=-10.0, input_size=[14,14])\n",
    "inpainting_attention2=InpaintingAttention(constant=-10.0, input_size=[14,8])\n",
    "\n",
    "if not no_attn:\n",
    "\n",
    "    # resizes from (3, 96) => (12, 12)\n",
    "    zero2one = nn.AvgPool2d(kernel_size=(1, 8))(\n",
    "            nn.Upsample(scale_factor=(4, 1), mode='nearest')(img_layer0)\n",
    "        )\n",
    "    img_layer1 = inpainting_attention1(img_layer1, zero2one)\n",
    "    print(\"0->1\",zero2one.shape, img_layer1.shape)\n",
    "\n",
    "    # resizes from (12, 12) => (12, 6)\n",
    "    one2two = nn.AvgPool2d(kernel_size=(1, 2))(img_layer1)\n",
    "\n",
    "    img_layer2 = inpainting_attention2(img_layer2, one2two)\n",
    "    print(\"1-2\",one2two.shape,img_layer2.shape)\n",
    "     \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Minibatch Discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16, 10)\n"
     ]
    }
   ],
   "source": [
    "def minibatch_output_shape(input_shape):\n",
    "    \"\"\" Computes output shape for a minibatch discrimination layer\"\"\"\n",
    "    shape = list(input_shape)\n",
    "    assert len(shape) == 3  # only valid for 3D tensors\n",
    "    return tuple(shape[:2])\n",
    "print(minibatch_output_shape((16,10,10)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 10])\n"
     ]
    }
   ],
   "source": [
    "def minibatch_discriminator(x):\n",
    "    # Expand dimensions and compute differences\n",
    "    diffs = x.unsqueeze(3) - x.permute(1, 2, 0).unsqueeze(0)\n",
    "    #print(diffs.shape)\n",
    "\n",
    "    # Compute the L1 norm\n",
    "    l1_norm = torch.sum(torch.abs(diffs), dim=2)\n",
    "    #print(l1_norm.shape)\n",
    "\n",
    "    # Compute the exponent of the negative L1 norm and sum across the batch\n",
    "    return torch.sum(torch.exp(-l1_norm), dim=2)\n",
    "\n",
    "\n",
    "# Example usage\n",
    "x = torch.randn(16,10,10)  # Example input tensor with shape [batch_size, channel, height, width]\n",
    "output = minibatch_discriminator(x)\n",
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "diffs (16, 10, 10, 16)\n",
      "l1 (16, 10, 16)\n",
      "(16, 10)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' \\n# Apply minibatch_discriminator using a Lambda layer\\noutput_tensor = Lambda(minibatch_discriminator_wrapper)(input_tensor)\\n\\n# Create model\\nmodel = Model(inputs=input_tensor, outputs=output_tensor)\\n\\n# Summary of the model\\nmodel.summary()\\n\\n# Create some sample data\\nbatch_size = 8\\nsample_data = np.random.rand(batch_size, 32, 32, 3)  # Example batch of 8 images\\n\\n# Predict using the model\\npredictions = model.predict(sample_data)\\n\\n# Print the shape of the output\\nprint(\"Output shape:\", predictions.shape)\\nprint(predictions) '"
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import keras.backend as K\n",
    "from keras.layers import Input, Lambda\n",
    "from keras.models import Model\n",
    "import numpy as np\n",
    "\n",
    "def minibatch_discriminator(x):\n",
    "    \"\"\" Computes minibatch discrimination features from input tensor x\"\"\"\n",
    "    diffs = K.expand_dims(x, 3) - K.expand_dims(K.permute_dimensions(x, [1, 2, 0]), 0)\n",
    "    l1_norm = K.sum(K.abs(diffs), axis=2)\n",
    "    return K.sum(K.exp(-l1_norm), axis=2)\n",
    "\n",
    "# Wrapper function to use with Keras Lambda layer\n",
    "def minibatch_discriminator_wrapper(x):\n",
    "    return minibatch_discriminator(x)\n",
    "\n",
    "# Define input tensor\n",
    "x = np.random.rand(16,10,10)  # Adjust input shape as needed\n",
    "diffs = K.expand_dims(x, 3) - K.expand_dims(K.permute_dimensions(x, [1, 2, 0]), 0)\n",
    "print(\"diffs\", diffs.shape)\n",
    "l1_norm = K.sum(K.abs(diffs), axis=2)\n",
    "print(\"l1\", l1_norm.shape)\n",
    "print(K.sum(K.exp(-l1_norm), axis=2).shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sparsity level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(14, 1)\n"
     ]
    }
   ],
   "source": [
    "#questa funzione √® indipendente dalla convenzione sulla posizione del canale\n",
    "def sparsity_output_shape(input_shape):\n",
    "    shape = list(input_shape)\n",
    "    return (shape[0], 1)\n",
    "\n",
    "print(sparsity_output_shape([16,10,10, 1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 1])\n"
     ]
    }
   ],
   "source": [
    "def sparsity_level(x):\n",
    "    # Get the shape of the input tensor\n",
    "    batch_size, channels, height, width = x.shape\n",
    "    \n",
    "    # Calculate the total number of elements per sample (ignoring batch dimension)\n",
    "    total = float(channels * height * width)\n",
    "    \n",
    "    # Count the number of non-zero elements along the (channels, height, width) dimensions\n",
    "    non_zero_counts = (x > 0.0).float().sum(dim=[1, 2, 3])\n",
    "    \n",
    "    # Reshape to a column vector [batch_size, 1]\n",
    "    non_zero_counts = non_zero_counts.view(batch_size, 1)\n",
    "    \n",
    "    # Calculate the sparsity level\n",
    "    sparsity = non_zero_counts / total\n",
    "    \n",
    "    return sparsity\n",
    "\n",
    "# Example usage:\n",
    "x = torch.randn(16,1,10,10)  # Example tensor with shape [batch_size, channels, height, width]\n",
    "sparsity = sparsity_level(x)\n",
    "print(sparsity.shape)  # Output shape should be [32, 1]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dense 3D layer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dense3D: input: torch.Size([10, 5]) kernel: torch.Size([3, 5, 4])\n",
      "torch.Size([10, 3, 4])\n",
      "(10, 3, 4)\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Note that in PyTorch, constraints and regularizers are typically handled separately, \n",
    "and the above example assumes simple weight initialization without custom constraints or regularizers. I\n",
    "changed the settings so that input_shape is a compulsory argument. I was not able to manage **kwargs\n",
    " \"\"\"\n",
    "\n",
    "class Dense3D(nn.Module):\n",
    "    \"\"\"\n",
    "    A 3D, trainable, dense tensor product layer.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, first_dim, last_dim, input_shape, activation=None, use_bias=True):\n",
    "        super(Dense3D, self).__init__()\n",
    "        self.first_dim = first_dim\n",
    "        self.last_dim = last_dim\n",
    "        self.activation = activation if activation else lambda x: x\n",
    "        self.use_bias = use_bias\n",
    "        self.input_shape = input_shape\n",
    "        self.input_dim = self.input_shape[-1]\n",
    "        # Apply constraints and regularizers (if any) after initialization\n",
    "        \n",
    "        # Initialize kernel (weights)\n",
    "        assert len(input_shape) >= 2\n",
    "        self.kernel = nn.Parameter(\n",
    "            torch.randn(first_dim, self.input_dim, last_dim)\n",
    "        )\n",
    "        nn.init.xavier_uniform_(self.kernel)\n",
    "   \n",
    "        # Initialize bias if use_bias is True\n",
    "        if use_bias:\n",
    "            self.bias = nn.Parameter(torch.zeros(first_dim, last_dim))\n",
    "        else:\n",
    "            self.bias = None\n",
    "        \n",
    "        nn.init.zeros_(self.bias)\n",
    "        \n",
    "\n",
    "    def forward(self, inputs):\n",
    "        # Compute the output\n",
    "        print(\"Dense3D: input:\", inputs.shape, \"kernel:\",self.kernel.shape)\n",
    "        out = torch.tensordot(inputs, self.kernel, dims=([1],[1]))\n",
    "        if self.use_bias:\n",
    "            out += self.bias\n",
    "        # Apply activation function\n",
    "        if self.activation:\n",
    "            out = self.activation(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        assert len(input_shape) == 2\n",
    "        return (input_shape[0], self.first_dim, self.last_dim)\n",
    "\n",
    "\n",
    "# Example usage\n",
    "input_tensor = torch.randn(10, 5)\n",
    "model = Dense3D(first_dim=3, last_dim=4, activation=F.relu, input_shape=(10,5))\n",
    "output = model(input_tensor)\n",
    "print(output.shape)\n",
    "print(model.compute_output_shape( (10,5) ) )\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input torch.Size([16, 1, 10, 10])\n",
      "after loc2 torch.Size([16, 16, 12, 7])\n",
      "before self.mbd torch.Size([16, 384])\n",
      "Dense3D: input: torch.Size([16, 1]) kernel: torch.Size([10, 1, 10])\n",
      "output: torch.Size([16, 394])\n"
     ]
    }
   ],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, mbd=False, sparsity=False, sparsity_mbd=False):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.mbd = mbd\n",
    "        self.sparsity = sparsity\n",
    "        self.sparsity_mbd = sparsity_mbd\n",
    "\n",
    "        self.conv1 = nn.Conv2d(1, 64, kernel_size=2, padding='same')\n",
    "        self.lrelu = nn.LeakyReLU()\n",
    "\n",
    "        self.zero_pad = nn.ZeroPad2d(1)\n",
    "        self.local_conv1 = LocallyConnected2d(64, 16, input_size=(12, 12), kernel_size=3, stride=(1,2), bias=0)\n",
    "        self.batch_norm1 = nn.BatchNorm2d(16)\n",
    "\n",
    "        self.local_conv2 = LocallyConnected2d(16, 8, input_size=(12, 7), kernel_size=2, bias=0)\n",
    "        self.batch_norm2 = nn.BatchNorm2d(8)\n",
    "\n",
    "        self.local_conv3 = LocallyConnected2d(8, 8, input_size=(13, 8), kernel_size=2, stride=(1,2), bias=0)\n",
    "        self.batch_norm3 = nn.BatchNorm2d(8)\n",
    "\n",
    "        self.flatten = nn.Flatten()\n",
    "\n",
    "        if self.mbd or self.sparsity or self.sparsity_mbd:\n",
    "            self.minibatch_featurizer = minibatch_discriminator\n",
    "            nb_features = 10\n",
    "            vspace_dim = 10\n",
    "            self.dense3d_1 = Dense3D(first_dim=nb_features, last_dim=vspace_dim, input_shape=(16,384) )\n",
    "            self.dense3d_2 = Dense3D(first_dim=nb_features, last_dim=vspace_dim, input_shape=(16,1))\n",
    "            self.activation_tanh = nn.Tanh()\n",
    "            self.sparsity_detector = sparsity_level \n",
    "\n",
    "    def forward(self, image):\n",
    "        #x = image.permute(0, 3, 1, 2)\n",
    "        x=image\n",
    "        x = self.conv1(x)\n",
    "        x = self.lrelu(x)\n",
    "\n",
    "        x = self.zero_pad(x) \n",
    "\n",
    "        x = self.local_conv1(x)\n",
    "        x = self.lrelu(x)\n",
    "        x = self.batch_norm1(x)\n",
    "\n",
    "        x = self.zero_pad(x)\n",
    "        print(\"after loc2\", x.shape)\n",
    "        \n",
    "        x = self.local_conv2(x)\n",
    "        x = self.lrelu(x)\n",
    "        x = self.batch_norm2(x)\n",
    "\n",
    "        x = self.zero_pad(x)\n",
    "    \n",
    "        x = self.local_conv3(x)\n",
    "        x = self.lrelu(x)\n",
    "        x = self.batch_norm3(x)\n",
    "\n",
    "        x = self.flatten(x)\n",
    "        print(\"before self.mbd\", x.shape)\n",
    "        if self.mbd or self.sparsity or self.sparsity_mbd:\n",
    "            features = [x]\n",
    "            if self.mbd:\n",
    "                K_x = self.dense3d_1(x)\n",
    "                print(\"K_x:\", K_x.shape)\n",
    "                print(\"after minibatch:\", self.minibatch_featurizer(K_x).shape)\n",
    "                features.append(self.activation_tanh(self.minibatch_featurizer(K_x)))\n",
    "                #print(\"features:\", features.shape)\n",
    "\n",
    "            if self.sparsity or self.sparsity_mbd:\n",
    "                empirical_sparsity = self.sparsity_detector(image)\n",
    "                if self.sparsity:\n",
    "                    features.append(empirical_sparsity)\n",
    "                if self.sparsity_mbd:\n",
    "                    K_sparsity = self.dense3d_2(empirical_sparsity)\n",
    "                    features.append(self.activation_tanh(self.minibatch_featurizer(K_sparsity)))\n",
    "\n",
    "            return torch.cat(features, dim=1)\n",
    "        else:\n",
    "            return x\n",
    "    \n",
    "discriminator = Discriminator(mbd=False, sparsity=False, sparsity_mbd=True)\n",
    "batch_size = 16\n",
    "input_channels = 1\n",
    "height = 10\n",
    "width = 10\n",
    "input_tensor = torch.randn(batch_size,input_channels, height, width)\n",
    "print(\"input\", input_tensor.shape)\n",
    "\n",
    "# Passare l'input attraverso il modello\n",
    "output_tensor = discriminator(input_tensor)\n",
    "print(\"output:\", output_tensor.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prima di spostare il canale prima di darlo alla rete\n",
    "input torch.Size([16, 10, 10, 1])\n",
    "after loc2 torch.Size([16, 16, 12, 7])\n",
    "before self.mbd torch.Size([16, 384])\n",
    "Dense3D: input: torch.Size([16, 384]) kernel: torch.Size([10, 384, 10])\n",
    "K_x: torch.Size([16, 10, 10])\n",
    "after minibatch: torch.Size([16, 10])\n",
    "output: torch.Size([16, 394])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Energy error and other functions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 0., 1.])\n",
      "tensor([0.1000, 0.0100, 0.1000])\n"
     ]
    }
   ],
   "source": [
    "def energy_error(requested_energy, received_energy):\n",
    "    # Compute the difference\n",
    "    difference = (received_energy - requested_energy) / 10000\n",
    "\n",
    "    # Determine if the energy is over the requested amount\n",
    "    over_energized = (difference > 0).float()\n",
    "    print(over_energized)\n",
    "\n",
    "    # Compute the penalties for too high and too low energy\n",
    "    too_high = 100 * torch.abs(difference)\n",
    "    too_low = 10 * torch.abs(difference)\n",
    "\n",
    "    # Compute the final energy error\n",
    "    return over_energized * too_high + (1 - over_energized) * too_low\n",
    "\n",
    "# Example usage:\n",
    "requested_energy = torch.tensor([100.0, 150.0, 200.0])\n",
    "received_energy = torch.tensor([110.0, 140.0, 210.0])\n",
    "error = energy_error(requested_energy, received_energy)\n",
    "print(error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def single_layer_energy(x):\n",
    "    shape = x.shape\n",
    "    return torch.sum(x, dim=tuple(range(1, len(shape)))).view(-1, 1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Loss: 3.4654\n",
      "Epoch [2/10], Loss: 3.4653\n",
      "Epoch [3/10], Loss: 3.4652\n",
      "Epoch [4/10], Loss: 3.4650\n",
      "Epoch [5/10], Loss: 3.4649\n",
      "Epoch [6/10], Loss: 3.4648\n",
      "Epoch [7/10], Loss: 3.4646\n",
      "Epoch [8/10], Loss: 3.4645\n",
      "Epoch [9/10], Loss: 3.4644\n",
      "Epoch [10/10], Loss: 3.4643\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "\n",
    "# Define the custom layer\n",
    "class ChannelSoftmax(nn.Module):\n",
    "    def forward(self, x):\n",
    "        e = torch.exp(x - torch.max(x, dim=-1, keepdim=True)[0])\n",
    "        s = torch.sum(e, dim=-1, keepdim=True)\n",
    "        return e / s\n",
    "\n",
    "# Build a simple model\n",
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Model, self).__init__()\n",
    "        self.fc = nn.Linear(10, 32)  # Dense layer with 10 input features and 32 output features\n",
    "        self.softmax = ChannelSoftmax()  # Custom softmax layer\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc(x))  # ReLU activation after the dense layer\n",
    "        x = self.softmax(x)  # Apply custom softmax\n",
    "        return x\n",
    "\n",
    "# Create an instance of the model\n",
    "model = Model()\n",
    "\n",
    "# Define loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()  # Cross-entropy loss for classification problems\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)  # Adam optimizer with a learning rate of 0.001\n",
    "\n",
    "# Generate some dummy data\n",
    "x_train = torch.randn(100, 10)  # Random input data with shape (100, 10)\n",
    "y_train = torch.randint(0, 32, (100,))  # Random labels with values between 0 and 31 (inclusive)\n",
    "\n",
    "# Train the model\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    # Forward pass\n",
    "    outputs = model(x_train)\n",
    "    # Compute loss\n",
    "    loss = criterion(outputs, y_train)\n",
    "    # Backward pass and optimization\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    # Print progress\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to define a model in keras. Three ways:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "# Define the model using the functional API\n",
    "inputs = tf.keras.Input(shape=(10,))\n",
    "x = layers.Dense(5, activation='relu')(inputs)\n",
    "x = layers.Dense(1)(x)\n",
    "outputs = layers.Activation('sigmoid')(x)\n",
    "\n",
    "# Create the model\n",
    "model = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation\n",
    "\n",
    "# Define the model using the sequential API\n",
    "model = Sequential([\n",
    "    Dense(5, input_shape=(10,), activation='relu'),\n",
    "    Dense(1),\n",
    "    Activation('sigmoid')\n",
    "])\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, Model\n",
    "\n",
    "# Define the model as a class\n",
    "class MyModel(Model):\n",
    "    def __init__(self):\n",
    "        super(MyModel, self).__init__()\n",
    "        self.dense1 = layers.Dense(5, activation='relu')\n",
    "        self.dense2 = layers.Dense(1)\n",
    "        self.activation = layers.Activation('sigmoid')\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x = self.dense1(inputs)\n",
    "        x = self.dense2(x)\n",
    "        return self.activation(x)\n",
    "\n",
    "# Instantiate the model\n",
    "model = MyModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 10)]              0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 32)                352       \n",
      "                                                                 \n",
      " lambda (Lambda)             (None, 32)                0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 352 (1.38 KB)\n",
      "Trainable params: 352 (1.38 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 54.7312\n",
      "Epoch 2/10\n",
      "7/7 [==============================] - 0s 733us/step - loss: 54.6636\n",
      "Epoch 3/10\n",
      "7/7 [==============================] - 0s 918us/step - loss: 54.6089\n",
      "Epoch 4/10\n",
      "7/7 [==============================] - 0s 918us/step - loss: 54.5577\n",
      "Epoch 5/10\n",
      "7/7 [==============================] - 0s 951us/step - loss: 54.5140\n",
      "Epoch 6/10\n",
      "7/7 [==============================] - 0s 918us/step - loss: 54.4742\n",
      "Epoch 7/10\n",
      "7/7 [==============================] - 0s 932us/step - loss: 54.4387\n",
      "Epoch 8/10\n",
      "7/7 [==============================] - 0s 910us/step - loss: 54.4089\n",
      "Epoch 9/10\n",
      "7/7 [==============================] - 0s 801us/step - loss: 54.3811\n",
      "Epoch 10/10\n",
      "7/7 [==============================] - 0s 822us/step - loss: 54.3552\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x29ca0b880>"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense, Lambda\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "# Define the custom channel_softmax function\n",
    "def channel_softmax(x):\n",
    "    e = K.exp(x - K.max(x, axis=-1, keepdims=True))\n",
    "    s = K.sum(e, axis=-1, keepdims=True)\n",
    "    return e / s\n",
    "\n",
    "# Create an input tensor\n",
    "input_tensor = Input(shape=(10,))\n",
    "\n",
    "# Define a simple model with a dense layer followed by the custom softmax layer\n",
    "x = Dense(32, activation='relu')(input_tensor)\n",
    "x = Lambda(channel_softmax)(x)\n",
    "\n",
    "# Create the model\n",
    "model = Model(inputs=input_tensor, outputs=x)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy')\n",
    "\n",
    "# Print the model summary\n",
    "model.summary()\n",
    "\n",
    "# Generate some dummy data\n",
    "import numpy as np\n",
    "x_train = np.random.rand(100, 10)\n",
    "y_train = np.random.randint(2, size=(100, 32))\n",
    "\n",
    "# Train the model\n",
    "model.fit(x_train, y_train, epochs=10, batch_size=16)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Inputs have incompatible shapes. Received shapes (32, 32, 6) and (32, 32, 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[103], line 42\u001b[0m\n\u001b[1;32m     39\u001b[0m constant_value \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m10\u001b[39m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;66;03m# Call the inpainting_attention function\u001b[39;00m\n\u001b[0;32m---> 42\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43minpainting_attention\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprimary\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcarryover\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconstant\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconstant_value\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;66;03m# Print the shape of the output tensor\u001b[39;00m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOutput shape:\u001b[39m\u001b[38;5;124m\"\u001b[39m, output\u001b[38;5;241m.\u001b[39mshape)\n",
      "Cell \u001b[0;32mIn[103], line 30\u001b[0m, in \u001b[0;36minpainting_attention\u001b[0;34m(primary, carryover, constant)\u001b[0m\n\u001b[1;32m     26\u001b[0m weights \u001b[38;5;241m=\u001b[39m Lambda(channel_softmax)(h)\n\u001b[1;32m     28\u001b[0m channel_sum \u001b[38;5;241m=\u001b[39m Lambda(K\u001b[38;5;241m.\u001b[39msum, arguments\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maxis\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mkeepdims\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28;01mTrue\u001b[39;00m})\n\u001b[0;32m---> 30\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m channel_sum(\u001b[43mmultiply\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweights\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/keras/src/layers/merging/multiply.py:84\u001b[0m, in \u001b[0;36mmultiply\u001b[0;34m(inputs, **kwargs)\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;129m@keras_export\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkeras.layers.multiply\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmultiply\u001b[39m(inputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     56\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Functional interface to the `Multiply` layer.\u001b[39;00m\n\u001b[1;32m     57\u001b[0m \n\u001b[1;32m     58\u001b[0m \u001b[38;5;124;03m    Example:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[38;5;124;03m        A tensor, the element-wise product of the inputs.\u001b[39;00m\n\u001b[1;32m     83\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 84\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mMultiply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/keras/src/layers/merging/base_merge.py:74\u001b[0m, in \u001b[0;36m_Merge._compute_elemwise_op_output_shape\u001b[0;34m(self, shape1, shape2)\u001b[0m\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     73\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m!=\u001b[39m j:\n\u001b[0;32m---> 74\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m     75\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInputs have incompatible shapes. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     76\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReceived shapes \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mshape1\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m and \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mshape2\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     77\u001b[0m             )\n\u001b[1;32m     78\u001b[0m         output_shape\u001b[38;5;241m.\u001b[39mappend(i)\n\u001b[1;32m     79\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mtuple\u001b[39m(output_shape)\n",
      "\u001b[0;31mValueError\u001b[0m: Inputs have incompatible shapes. Received shapes (32, 32, 6) and (32, 32, 2)"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras.layers import concatenate, ZeroPadding2D, Lambda, multiply, LocallyConnected2D\n",
    "from keras.initializers import Constant\n",
    "from keras.models import Model\n",
    "from keras import backend as K\n",
    "\n",
    "def inpainting_attention(primary, carryover, constant=-10):\n",
    "\n",
    "    def _initialize_bias(const=-5):\n",
    "        def _(shape, dtype=None):\n",
    "            assert len(shape) == 3, 'must be a 3D shape'\n",
    "            x = np.zeros(shape)\n",
    "            x[:, :, -1] = const\n",
    "            return x\n",
    "        return _\n",
    "\n",
    "    x = concatenate([primary, carryover], axis=-1)\n",
    "    h = ZeroPadding2D((1, 1))(x)\n",
    "    lcn = LocallyConnected2D(\n",
    "        filters=2,\n",
    "        kernel_size=(3, 3),\n",
    "        bias_initializer=_initialize_bias(constant)\n",
    "    )\n",
    "\n",
    "    h = lcn(h)\n",
    "    weights = Lambda(channel_softmax)(h)\n",
    "\n",
    "    channel_sum = Lambda(K.sum, arguments={'axis': -1, 'keepdims': True})\n",
    "\n",
    "    return channel_sum(multiply([x, weights]))\n",
    "\n",
    "\n",
    "# Example usage\n",
    "# Create sample primary and carryover tensors\n",
    "primary = np.random.randn(1, 32, 32, 3)  # Sample primary tensor with shape (1, 32, 32, 3)\n",
    "carryover = np.random.randn(1, 32, 32, 3)  # Sample carryover tensor with shape (1, 32, 32, 3)\n",
    "\n",
    "# Set constant value for bias initialization\n",
    "constant_value = -10\n",
    "\n",
    "# Call the inpainting_attention function\n",
    "output = inpainting_attention(primary, carryover, constant=constant_value)\n",
    "\n",
    "# Print the shape of the output tensor\n",
    "print(\"Output shape:\", output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[0. 0. 1.]\n",
      "  [0. 0. 1.]\n",
      "  [0. 0. 1.]]\n",
      "\n",
      " [[0. 0. 1.]\n",
      "  [0. 0. 1.]\n",
      "  [0. 0. 1.]]\n",
      "\n",
      " [[0. 0. 1.]\n",
      "  [0. 0. 1.]\n",
      "  [0. 0. 1.]]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def _(shape, dtype=None):\n",
    "    assert len(shape) == 3, 'must be a 3D shape'\n",
    "    x = np.zeros(shape)\n",
    "    x[:, :, -1] = 1\n",
    "    return x\n",
    "\n",
    "\n",
    "print(_([3,3,3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.modules.utils import _pair\n",
    "\n",
    "\"\"\" I modified locally connected2d so that bias is a float variable: if it is set to zero no bias is introduced, if it is set to \n",
    "any other number, that will be the bias\n",
    "I also made another change: provide the input size together with all the other parameters of the convolution. It will authomatically \n",
    "compute the output dimension and prepare all the weights.\n",
    "For now I assume that kernel size, padding, and stride are the same in both h and w dimensions \n",
    "In order to stack many of those layers I also return the output shape, which will be the input shape of the next layer\n",
    "\"\"\"\n",
    "\n",
    "\"\"\" #filters = #out_channels \"\"\"\n",
    "\n",
    "class LocallyConnected2d(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, input_size, kernel_size, bias, stride=1, padding=0, dilation=1):\n",
    "        super(LocallyConnected2d, self).__init__()\n",
    "        output_size_0=int(1+ (input_size[0] + 2*padding - dilation*(kernel_size-1)-1)/stride)\n",
    "        output_size_1=int(1+ (input_size[1] + 2*padding - dilation*(kernel_size-1)-1)/stride)\n",
    "        self.weight = nn.Parameter(\n",
    "            torch.randn(1, out_channels, in_channels, output_size_0, output_size_1, kernel_size**2)\n",
    "        )\n",
    "        if bias:\n",
    "            self.bias = nn.Parameter(\n",
    "                torch.full((1, out_channels, output_size_0, output_size_1), bias, requires_grad=True)\n",
    "            )\n",
    "        else:\n",
    "            self.register_parameter('bias', None)\n",
    "        self.kernel_size = _pair(kernel_size)\n",
    "        self.stride = _pair(stride)\n",
    "\n",
    "    def forward(self, x):\n",
    "        _, c, h, w = x.size()\n",
    "        kh, kw = self.kernel_size\n",
    "        dh, dw = self.stride\n",
    "        x = x.unfold(2, kh, dh).unfold(3, kw, dw)\n",
    "        x = x.contiguous().view(*x.size()[:-2], -1)\n",
    "        # Sum in in_channel and kernel_size dims       \n",
    "        out = (x.unsqueeze(1) * self.weight).sum([2, -1])\n",
    "\n",
    "        if self.bias is not None:\n",
    "            out += self.bias \n",
    "        return out, out.shape\n",
    "    \n",
    "        \n",
    "   \n",
    "batch_size = 5\n",
    "in_channels = 3\n",
    "h, w = 96,3\n",
    "x = torch.randn(batch_size, in_channels, h, w)\n",
    "print(\"x\", x.shape)\n",
    "\n",
    "# Create layer and test if backpropagation works\n",
    "\n",
    "out_channels = 2\n",
    "kernel_size = 3\n",
    "stride = 1\n",
    "\n",
    "\n",
    "conv = LocallyConnected2d(\n",
    "    in_channels, out_channels, [h,w], kernel_size, bias=6.0, stride=stride)\n",
    "print(\"bias\", conv.bias.shape)\n",
    "out = conv(x)\n",
    "print(\"out\", out[0].shape)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
